
# coding: utf-8

# In[82]:


#Importing all the libraries we will need upfront
import keras
import gensim
import nltk
import sklearn
import pandas as pd
import numpy as np
import matplotlib

import re
import codecs
import itertools


# In[83]:


swa = pd.read_csv("p1226jlri4d_50s_pver-i_cal_swa.csv", encoding = "ISO-8859-1")
swa.columns=['Comment', 'Relevant or Irrelevant']
swa.head()


# In[84]:


swa.tail()


# In[85]:


swa.describe()


# In[86]:


#Data Cleansing
def standardize_text(df, text_field):
    df[text_field] = df[text_field].str.lower()
    df[text_field] = df[text_field].apply(lambda elem: re.sub(r"http\S+", "", elem))  # get rid of URLs
    return df

clean_swa = standardize_text(swa, "Comment")


# In[87]:


clean_swa.head()


# In[88]:


clean_swa.tail()


# In[89]:


#Checking class balance
clean_swa.groupby("Relevant or Irrelevant").count()


# In[90]:


#Our data is clean, now let's inspect it a little bit more.
#Tokenizing sentences to a list of separate words
    


from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')

clean_swa["tokens"] = clean_swa["Comment"].apply(tokenizer.tokenize)
clean_swa.head()


# In[91]:


#Analysing Comment lengths

all_words = [word for tokens in clean_swa["tokens"] for word in tokens]
sentence_lengths = [len(tokens) for tokens in clean_swa["tokens"]]
VOCAB = sorted(list(set(all_words)))
print("%s words total, with a vocabulary size of %s" % (len(all_words), len(VOCAB)))
print("Max sentence length is %s" % max(sentence_lengths))


# In[92]:


#Inspecting our data a little more to validate results

import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10, 10)) 
plt.xlabel('Sentence length')
plt.ylabel('Number of sentences')
plt.hist(sentence_lengths)
plt.show()


# In[93]:


#Preparing our data: Train/Test split

from sklearn.model_selection import train_test_split

list_corpus = clean_swa["Comment"]
list_labels = clean_swa["Relevant or Irrelevant"]

X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=40)

print("Training set: %d samples" % len(X_train))
print("Test set: %d samples" % len(X_test))


# In[94]:


print (X_train[:10])


# In[95]:


print (y_train[:10])


# In[96]:


#Embedding using BoW (Bag of words)

from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\w+')

bow = dict()
bow["train"] = (count_vectorizer.fit_transform(X_train), y_train)
bow["test"]  = (count_vectorizer.transform(X_test), y_test)


# In[97]:


print(bow["train"][0].shape)
print(bow["test"][0].shape)


# In[98]:


#Embedding using TFIDF BoW (Term Frequency, Inverse Document Frequency)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\w+')

tfidf = dict()
tfidf["train"] = (tfidf_vectorizer.fit_transform(X_train), y_train)
tfidf["test"]  = (tfidf_vectorizer.transform(X_test), y_test)


# In[99]:


print(tfidf["train"][0].shape)
print(tfidf["test"][0].shape)


# In[100]:


#Word2Vec - Capturing semantic meaning

word2vec_path = "GoogleNews-vectors-negative300.bin"
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)


# In[101]:


#Average word2vec scores of all words in our sentence

def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):
    if len(tokens_list)<1:
        return np.zeros(k)
    if generate_missing:
        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]
    else:
        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]
    length = len(vectorized)
    summed = np.sum(vectorized, axis=0)
    averaged = np.divide(summed, length)
    return averaged

def get_word2vec_embeddings(vectors, clean_swa_tokens, generate_missing=False):
    embeddings = clean_swa_tokens.apply(lambda x: get_average_word2vec(x, vectors, 
                                                                                generate_missing=generate_missing))
    return list(embeddings)


# In[102]:


embeddings = get_word2vec_embeddings(word2vec, clean_swa['tokens'])
X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(embeddings, list_labels, 
                                                                    test_size=0.2, random_state=40)

w2v = dict()
w2v["train"] = (X_train_w2v, y_train_w2v)
w2v["test"]  = (X_test_w2v, y_test_w2v)


# In[103]:


#On to the Classification
#Functions to check accuracy of our trained models

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

def get_metrics(y_test, y_predicted):  
    # true positives / (true positives+false positives)
    precision = precision_score(y_test, y_predicted, pos_label=None,
                                    average='weighted')             
    # true positives / (true positives + false negatives)
    recall = recall_score(y_test, y_predicted, pos_label=None,
                              average='weighted')
    
    # harmonic mean of precision and recall
    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')
    
    # true positives + true negatives/ total
    accuracy = accuracy_score(y_test, y_predicted)
    return accuracy, precision, recall, f1


# In[104]:


#Confusion Matrix to see false positives and false negatives

from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.winter):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=30)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, fontsize=20)
    plt.yticks(tick_marks, classes, fontsize=20)
    
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.

    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", 
                 color="white" if cm[i, j] < thresh else "black", fontsize=40)
    
    plt.tight_layout()
    plt.ylabel('True label', fontsize=30)
    plt.xlabel('Predicted label', fontsize=30)

    return plt


# In[105]:


#Classifier: Logistic Regression

from sklearn.linear_model import LogisticRegression

lr_classifier = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', 
                         multi_class='multinomial', random_state=40)


# In[106]:


#Classifier: Linear Support Vector Machine 

from sklearn.svm import LinearSVC

lsvm_classifier = LinearSVC(C=1.0, class_weight='balanced', multi_class='ovr', random_state=40)


# In[107]:


#Classifier: Naive Bayes

from sklearn.naive_bayes import MultinomialNB

nb_classifier = MultinomialNB()


# In[108]:


#Choosing the Embedding: 

embedding = bow                  # bow | tfidf | w2v


# In[109]:


#Choosing the Classifier:

classifier = lr_classifier     # lr_classifier | lsvm_classifier | nb_classifier


# In[110]:


#Fit and Predict on our Test Data

classifier.fit(*embedding["train"])
y_predict = classifier.predict(embedding["test"][0])


# In[111]:


#Our score on the model

accuracy, precision, recall, f1 = get_metrics(embedding["test"][1], y_predict)
print("accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f" % (accuracy, precision, recall, f1))


# In[112]:


#Confusion matrix on our predicted test data to understand false positive and false negative:

cm = confusion_matrix(embedding["test"][1], y_predict)
fig = plt.figure(figsize=(10, 10))
plot = plot_confusion_matrix(cm, classes=['Irrelevant','Relevant','Unsure'], normalize=False, title='Confusion matrix')
plt.show()


# In[113]:


# Read the test data
test_X = pd.read_csv('comveh_bci_swa_109_54.csv')

test_corpus = test_X["Comment"]

# tokenize the test_corpus, used in Word2Vec
test_corpus_tokens = test_corpus.apply(tokenizer.tokenize)

test_Id = test_X["Id"]
test_Output = test_X["Relevant or Irrelevant"]


# In[114]:


# Vectorize the comments using your Embedding of choice
vectorized_text = dict()
vectorized_text['test']  = (count_vectorizer.transform(test_corpus))  # see options in the above cell


# In[115]:


embedding = vectorized_text                
classifier = lr_classifier     # lr_classifier | lsvm_classifier | nb_classifier
predicted_sentiment = classifier.predict(embedding['test']).tolist()

results = pd.DataFrame(
    {'Id': test_Id,
     'Expected': predicted_sentiment,
     'Relevant or Irrelevant': test_Output
    })

#Printing results to_csv

results.to_csv('swa_analysed_report.csv', index=False)


# In[ ]:




